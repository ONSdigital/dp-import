#!/usr/bin/env bash

usage(){
    echo "${0##*/} args..."
    cat <<'EOFusage'

ID arguments in the following can usually be abbreviated (save typing).
'args' can be:

    create                  create an import-job using json to the ImportAPI
    create_filt [$iid]      create a filter-job with instance_id in json to FilterAPI
    submit                  change an import-job to state:submitted on the ImportAPI
    job       $jid          get an import-job from the ImportAPI
    jobs                    get a list of import-jobs from the ImportAPI
    inst      [$iid]        get an instance (or all instances) from the DatasetAPI
    filt      [$fid]        get a filter job (or all) from the FilterAPI

    dset      [$dsid]                               get a dataset (or all datasets) from the DatasetAPI
    dset      $dsid [ $eid [ $vid [ $dimid ] ] ]    get a dataset/edition/version/dimension ('-' adds next sub-uri w/o id)
    assoc     $dsid $eid $vid                       change a dataset state=associated on the DatasetAPI

    submitted               state-filtered: import-jobs<ImportAPI; datasets,instances<DatasetAPI; recipes<RecipeAPI
    created                 as per 'submitted'
    associated|published    as per 'submitted', only checks datasets

    dim_x     $iid          create a kafka message consumed by the dimension-extractor

    db_wipe                 wipe all DBs (after confirmation!)
    db                      display all DBs

    tot_ob    $iid $tot     update total_observations via mongo
    db_ins_ob $iid $inc     increment total_inserted_observations via mongo
    ins_ob    $iid $inc     create kafka message to increment total_inserted_observations

    inst_to_job  $iid       undoc'd utilities...
    inst_to_cols $iid
    inst_to_dims $iid
    inst_to_col1 $iid
    inst_like    $iid
    job_to_file  $jid

    ok|health               query the /healthcheck endpoints

    neo                     more undoc'd utilities...
    neo_count
    neo_nodes
    cypher_obs
    mongo_all
    mongo_wipe
    add_file
EOFusage
}

. g_lib.sh

IMPORT_API_ADDR=${IMPORT_API_ADDR:-localhost:21800}
IMPORT_API_AUTH_TOKEN=${IMPORT_API_AUTH_TOKEN:-FD0108EA-825D-411C-9B1D-41EF7727F465}
DATASET_API_ADDR=${DATASET_API_ADDR:-localhost:22000}
DATASET_API_AUTH_TOKEN=${DATASET_API_AUTH_TOKEN:-$IMPORT_API_AUTH_TOKEN}
FILTER_API_ADDR=${FILTER_API_ADDR:-localhost:22100}
FILTER_API_AUTH_TOKEN=${FILTER_API_AUTH_TOKEN:-$IMPORT_API_AUTH_TOKEN}
RECIPE_API_ADDR=${RECIPE_API_ADDR:-localhost:22300}
ONS_DP_SRC_ROOT=${ONS_DP_SRC_ROOT:-$HOME/go/src/github.com/ONSdigital}

f_count=0

import_api_dir=$ONS_DP_SRC_ROOT/dp-import-api
filter_api_dir=$ONS_DP_SRC_ROOT/dp-filter-api

# postgres (now unused, but see $legacy)
import_db_name=ImportJobs
filter_db_name=FilterJobs
# set this non-empty if using postgres (blank => mongo)
legacy=

# neo4j
cypher_client=neo4j-client
cypher_args=neo4j://neo4j@localhost:7687
cypher_client=cypher-shell
cypher_args="--format plain"

# mongo (see $legacy)
mongo_imports_db=imports
mongo_imports_c=imports
mongo_datasets_db=datasets
mongo_datasets_c=datasets
mongo_instances_db=$mongo_datasets_db
mongo_instances_c=instances
mongo_dimensions_db=$mongo_datasets_db
mongo_dimensions_c=dimensions
mongo_editions_db=$mongo_datasets_db
mongo_editions_c=editions
mongo_versions_db=$mongo_datasets_db
mongo_versions_c=versions
mongo_cmd="mongo --quiet"
mongo_js='var all={"imports":{"db":"'"$mongo_imports_db"'","coll":"'"$mongo_imports_c"'"},"instances":{"db":"'"$mongo_instances_db"'","coll":"'"$mongo_instances_c"'"},"datasets":{"db":"'"$mongo_datasets_db"'","coll":"'"$mongo_datasets_c"'"},"dimensions":{"db":"'"$mongo_dimensions_db"'","coll":"'"$mongo_dimensions_c"'"},"editions":{"db":"'"$mongo_editions_db"'","coll":"'"$mongo_editions_c"'"},"versions":{"db":"'"$mongo_versions_db"'","coll":"'"$mongo_versions_c"'"}}'

# s3_file='s3://dp-dimension-extractor/OCIGrowth.csv'
# s3_file='s3://dp-dimension-extractor/EXAMPLE_V4-coicopcomb.csv'
# s3_file='https://s3-eu-west-1.amazonaws.com/dp-frontend-florence-file-uploads/2470609-EXAMPLE_V4-coicopcomb-inc-geo-codecsv'
# s3_file='s3://dp-dimension-extractor/EXAMPLE_V4-Internal Migration Done.csv'
# s3_file='https://s3-eu-west-1.amazonaws.com/dp-frontend-florence-file-uploads/2470609-EXAMPLE_V4-coicopcomb-inc-geo-code2csv'
# no longer there...  s3_file='s3://dp-frontend-florence-file-uploads/2470609-EXAMPLE_V4-coicopcomb-inc-geo-code2csv'
s3_file='s3://dp-dimension-extractor/EXAMPLE_V4-coicopcomb-inc-geo-code.csv'

#       create_json='{"recipe":"v4","state":"created","files":[{"alias_name":"v4","url":"s3://dp-publish-content-test/OCIGrowth.csv"}]}'
#       create_json='{"recipe":"v4","files":[{"alias_name":"v4","url":"s3://dp-publish-content-test/OCIGrowth.csv"}]}'
# XXX magic XXX         b944be78-f56d-409b-9ebd-ab2b77ffe187 for the import-api to send direct to dimension-extractor
create_json='{"recipe":"b944be78-f56d-409b-9ebd-ab2b77ffe187","files":[{"alias_name":"v4","url":"'"$s3_file"'"}]}'

create_filt_json='{"instance_id":"<iid>"}'

#       submit_json='{"id":"<job_id>","state":"submitted"}'
submit_json='{"state":"submitted"}'
assoc_json='{"state":"associated","collection_id":"95c4669b-3ae9-4ba7-b690-87e890a1c543"}'

add_file_json='{"alias_name":"v4-2","url":"'"$s3_file"'_22"}'

curly_put (){ curly -X PUT  "$@";}
curly_post(){ curly -X POST "$@";}
curly(){
        local token=$IMPORT_API_AUTH_TOKEN
        if   [[ ${@:$#} =~ $DATASET_API_ADDR* ]]; then token=$DATASET_API_AUTH_TOKEN
        elif [[ ${@:$#} =~ $FILTER_API_ADDR*  ]]; then token=$FILTER_API_AUTH_TOKEN
        elif [[ ${@:$#} =~ $RECIPE_API_ADDR*  ]]; then token=$RECIPE_API_AUTH_TOKEN
        fi
        curl -sSH "Content-type: application/json" -H "Internal-Token: $token" "$@"
}

show_r_js="function show_(r){if(r==null){print('');return;};if(r.hasNext != undefined){while(r.hasNext()){printjson(r.next());}}else{printjson(r)}}"
ppj_js="function p(r){print(r);};function pj(r){printjson(r)}"

# unused
cypher_obs1() {
        local id=$1; id=$(like_to_id instance $id)
        echo 'MATCH (time:`_'"$id"'_Time`),
    (geo:`_'"$id"'_Geography`),
    (aggr:`_'"$id"'_Aggregate`)
WHERE aggr.value = "cpi1dim1G10100"
    AND time.value = "Jan-96"
    AND geo.value = "K02000001"
WITH time,geo,aggr
MATCH (o:`_'"$id"'_observation`), (o)-[:isValueOf]->(time),
    (o)-[:isValueOf]->(geo),
    (o)-[:isValueOf]->(aggr)
RETURN o
LIMIT 2;'
}

cypher_obs() {
        local id=$(like_to_id instance $1) col_count=0
        local cols=$(inst_to_dims $id)
        declare -A col_as=()
        echo -en "MATCH\t"
        col_count=0; for col in $cols; do (( ++col_count > 1 )) && echo -en ",\n\t"
                local col_key=${col:0:3} col_len=3; while [[ -n "${col_as[$col_key]}" ]]; do col_len++; col_key=${col:0:$col_len}; done; col_as[$col]=$col_key
                echo -n "(${col_key}:\`_${id}_${col}\`)"
        done
        echo -en "\nWHERE\t    "
        col_count=0; for col in $cols; do (( ++col_count > 1 )) && echo -en '\n\tAND '
                echo -n "${col_as[$col]}"'.value = ""'
        done
        echo -en "\nWITH\t"
        col_count=0; for col in $cols; do (( ++col_count > 1 )) && echo -n ", "
                echo -n "${col_as[$col]}"
        done
        echo -en "\nMATCH\t(obs:\`_${id}_observation\`)"
        for col in $cols; do
                echo -en ",\n\t(obs)-[:isValueOf]->(${col_as[$col]})"
        done
        echo -e "\nRETURN obs\nLIMIT 2;"
}

neo(){
        local id=$1; id=$(like_to_id instance $id)
        cypher_obs $id
        yorn Run above against Neo4j... || return
        cypher_obs $id | $cypher_client $cypher_args
        # yorn Pretty print... || return
        # cypher_obs | $cypher_client $cypher_args | perl -nE '$.==2 and s/""/"/g and s/(^"|"$)//g and print'
}

neo_do(){ while [[ -n $1 ]]; do yorn --x $cypher_client $cypher_args "$1" || return; shift; done; }
neo_count(){ local id=$(like_to_id instance $1); neo_do "MATCH (o:\`_${id}_observation\`) RETURN COUNT(o);"; }
neo_wipe(){
        yorn --xc brew services stop neo4j
        yorn --xc rm -rf /usr/local/Cellar/neo4j/*/libexec/data/databases/graph.db && brew services start neo4j
        neo_do "MATCH (n) DETACH DELETE n;"
}
#   neo_wipe(){ neo_do "MATCH (n) OPTIONAL MATCH (n)-[r]-() DELETE r,n;"; }
#   neo_wipe(){ neo_do "MATCH ()-[r]-() DELETE r;" "MATCH (n) DELETE n;"; }
#   neo_nodes(){ neo_do "MATCH node(*) RETURN COUNT(n) AS node_count;"; }
neo_nodes(){ local id=$(like_to_id instance $1); [[ -z $id ]] && return 1; neo_do "MATCH (n:\`_${id}_observation\`) RETURN COUNT(n) AS obs_count;" "MATCH (n) RETURN COUNT(n) AS node_count;"; }
neo_all(){ neo_nodes . || return 1; }

create() {
        local url= instance_id=
        g_info "New import job... " >&2
        echo $create_json|jq .
        yorn POST above to API $IMPORT_API_ADDR/jobs || return
        local json= out=$(curly_post -id "$create_json" $IMPORT_API_ADDR/jobs)
        get_json "$out"
        if [[ -n $legacy ]]; then
                instance_id=$(echo "$json" | jq -r '.instances[0].id')
        else
                instance_id=$(echo "$json" | jq -r '.links.instances[0].id')
                [[ $instance_id == null ]] && instance_id=
        fi
        g_warn "i_id=:$instance_id:"
        [[ -n $instance_id ]] && inst $instance_id
}

create_filt() {
        local instance_id=$(like_to_id instance "$1"); shift
        local create_filt_json_iid=${create_filt_json//<iid>/$instance_id}
        echo $create_filt_json_iid|jq .
        yorn "Create filter job... send above: POST $FILTER_API_ADDR/filters" || return
        local json= out=$(curly_post -id "$create_filt_json_iid" $FILTER_API_ADDR/filters)
        get_json "$out"
        # instance_id=$(echo "$json" | jq -r '.links.instances[0].id')
        # [[ $instance_id == null ]] && instance_id=
        # g_warn "i_id=:$instance_id:"
        # [[ -n $instance_id ]] && inst $instance_id
}
create_filt_opt() {
        local filt_id=$(like_to_id filter "$1"); shift
        local json= out=$(curly_post -id "$create_filt_json_iid" $FILTER_API_ADDR/filters)
        get_json "$out"
}

gen_url(){
        local sub_url=$1; shift
        if [[ $sub_url != */* ]]; then
                if   [[ :filters:instances:datasets:dimensions:editions:versions:contacts: == *:$sub_url:* ]]; then
                        local next_uri= # colon-separated and -terminated
                        if   [[ :datasets: == *:$sub_url:* && -n $1 ]]; then
                                next_uri=$sub_url:editions:versions:dimensions:
                                sub_url=$DATASET_API_ADDR
                        elif [[ $sub_url == filters ]]; then
                                if [[ -n $1 ]]; then
                                        next_uri=$sub_url:dimensions:options:
                                        sub_url=$FILTER_API_ADDR
                                else
                                        sub_url=$FILTER_API_ADDR/$sub_url
                                fi
                        else
                                sub_url=$DATASET_API_ADDR/$sub_url
                        fi
                        while [[ -n $next_uri && -n $1 ]]; do
                                sub_url=$sub_url/${next_uri%%:*}
                                next_uri=${next_uri#*:}
                                [[ $1 != '-' ]] && sub_url=$sub_url/$1
                                shift
                        done
                elif [[ $sub_url == recipes ]]; then
                        sub_url=$RECIPE_API_ADDR/$sub_url
                else
                        sub_url=$IMPORT_API_ADDR/$sub_url # default to ImportAPI
                fi
        fi
        [[ -n $1 ]] && sub_url=$sub_url/$1
        echo $sub_url
}
api_raw(){      local method=$1 url=$2; local out=$(curly  -X $method $url); echo "$out"; }
api_raw_hdrs(){ local method=$1 url=$2; local out=$(curly -iX $method $url); echo "$out"; }

getter() {
        local db_type=$1 id_or_url=$2; shift 2
        if [[ -n $id_or_url ]]; then id_or_url=$(like_to_id $db_type $id_or_url); [[ -z $id_or_url ]] && return 1; fi
        id_or_url=$(gen_url ${db_type}s "$id_or_url" "$@")
        g_info "Getting ${db_type} info... $id_or_url " >&2
        api_raw GET $id_or_url
}
inst() { local url=$1; shift; getter instance   $url | jq .; }
dset() {                      getter dataset    "$@" | jq .; }
filt() {                      getter filter     "$@" | jq .; }
inst_to_job()  { getter instance "$@" | jq -r .links.job.id -; }
inst_to_cols() { getter instance "$@" | jq -r .headers -; }
inst_to_all()  { getter instance "$@" | perl -MJSON -E '{local $/; $h=decode_json <>; $col=0; for my $h (@{ $hash->{headers} }){ $col++; if ($col==1) { $x=($h =~ s/V4_//r); say "$col\tx=$x\t$h" if 0; next; } if ($col<=$skip) { say "$col\tskip\y$h" if 0; next; } say $h if ($col-$x)%2; } }'; }
inst_to_dims() { local col1=$(inst_to_col1 "$@"); getter instance "$@" | perl -MJSON -E '{local $/; $h=decode_json <>; for(my $c=1+'"$col1"'; $c<@{$h->{headers}}; $c+=2){ say $h->{headers}->[$c]; } }'; }
inst_to_col1() { getter instance "$@" | perl -MJSON -E '{local $/; $h=decode_json <>; say 1+($h->{headers}->[0] =~ s/V4_//r);}'; }
job_to_file() { getter job "$@" | jq -r '.files[0].url' -; }
jobs_(){ local url=$1; shift; getter job $url | jq .; }
recipe() { local url=$1; shift; getter recipe $url | jq .; }

submit(){ local url=$1; shift; url=$(like_to_id job $url); url=$(gen_url jobs $url); g_info "Submitting import (state=>submitted): PUT $url " >&2
        local out=$(curly_put -id "$submit_json" $url)
        get_json "$out"
}

assoc(){ local url=$1; shift; url=$(like_to_id dataset $url); url=$(gen_url datasets $url "$@"); g_info "Associating dataset (state=>associated): PUT $url " >&2
        local out=$(curly_put -id "$assoc_json" $url)
        get_json "$out"
}

add_file(){ local url=$1; shift; url=$(like_to_id job $url); url=$(gen_url jobs $url)/files; g_info "Adding file: PUT $url" >&2
        echo "$add_file_json" | jq .
        local out=$(curly_put -id "$add_file_json" $url)
        get_json "$out"
}

get_json(){
        local out=$1; shift
        echo "out=:$out:"
        json=$(echo "$out" | perl -nE 'if (/^\s*$/...eof()){ s/^[\r\n]+//m; print;}')
        echo "$json" | jq .
}
states(){ local url= otype=$1 states=$2; shift 2; url=$(gen_url $otype)\?state=$states; g_info "Getting $otype: GET $url" >&2
        local out=$(curly -i "$url")
        get_json "$out"
}

do_mongo_raw() {
        local temp_js=/tmp/temp_js_$$_$f_count.js res=0 js=$1; shift; let f_count++
        echo "$js" > $temp_js # grep . $temp_js /dev/null >&2
        if ! eval $mongo_cmd "$@" $temp_js | perl -pE 's/ObjectId\("([^"]+)"\)/"oId_$1"/g;'; then res=$? ;grep --color . $temp_js /dev/null >&2; fi
        rm $temp_js
        return $res
}
do_mongo_r() { local js=$1; shift; do_mongo_raw "$mongo_js;$ppj_js;$show_r_js;$js;" "$@"; }
do_mongo() { local js=$1; shift; do_mongo_raw "$mongo_js;$ppj_js;r=$js;$show_r_js;show_(r);" "$@"; }
do_mongo_all() { local js=$1; shift; do_mongo_raw "$mongo_js;$ppj_js;$show_r_js;for(var d in all){var db=db.getSiblingDB(all[d].db);coll=db.getCollection(all[d].coll);$js;}" "$@"; }
do_mongo_imports() { do_mongo "$@" $mongo_imports_db; }
do_mongo_datasets() { do_mongo "$@" $mongo_datasets_db; }
like_to_id() {
        if [[ $2 == */* ]]; then echo $1; g_warn l2id "$@" >&2; return; fi # poss useless?
        if [[ -n $legacy ]]; then
                psql -U dp $import_db_name -Atc "SELECT ${1}id FROM ${1}s WHERE ${1}id LIKE '$2%'"
        elif [[ $1 == recipe ]]; then
                api_raw GET $RECIPE_API_ADDR/recipes | jq -r '.items[0].id'
        elif [[ :dataset: == *:$1:* ]]; then
                do_mongo_datasets "db.getCollection('$mongo_datasets_c').findOne({_id:/^$2/}, {_id:1})" | jq -r ._id
        elif [[ :filter: == *:$1:* ]]; then
                # do_mongo_datasets "db.getCollection('$mongo_filter_c').findOne({_id:/^$2/}, {_id:1})" | jq -r ._id
                psql -U dp $filter_db_name -Atc "SELECT ${1}JobId FROM ${1}s WHERE ${1}JobId LIKE '$2%'"
        elif [[ :instance: == *:$1:* ]]; then
                g_warn iid=$2
                do_mongo_datasets "db.getCollection('$mongo_instances_c').findOne({id:/^$2/}, {id:1})" | jq -r .id
        else # job
                # 'print(JSON.stringify(db.s3.getIndexes(), null, 8),JSON.stringify(db.meta.getIndexes(), null, 8))'
                # echo do_mongo_imports "db.getCollection('$mongo_imports_c').find({id:/^$2/}, {id:1})"
                # g_info $mongo_imports_c $2 $(do_mongo_imports "db.getCollection('$mongo_imports_c').find({id:/^$2/}, {id:1})")  >&2
                do_mongo_imports "db.getCollection('$mongo_imports_c').findOne({id:/^$2/}, {id:1})" | jq -r .id
        fi
}
inst_like(){
        if [[ -n $legacy ]]; then
                psql -U dp $import_db_name -Atc 'SELECT instance FROM instances WHERE instanceid LIKE '"'$1%'" | jq .
        else
                do_mongo_datasets "db.getCollection('$mongo_instances_c').find({id:/^$2/})" | jq .
        fi
}
tot_ob() {
        local id=$1 obs=$2; shift 2; [[ -z $obs ]] && obs=1000; id=$(like_to_id instance $id)
        inst $id
        if [[ -n $legacy ]]; then
                psql -U dp $import_db_name -c 'UPDATE instances SET instance=instance || JSONB('"'"'{"total_observations":'"${obs}}'"') WHERE instanceid = '"'$id'"
        else
                do_mongo_datasets "db.getCollection('$mongo_instances_c').update({id:'$id'}, {\$set:{total_observations:$obs}})" | jq .
        fi
        inst $id
}
db_ins_ob() {
        local id=$1 obs=$2; shift 2; [[ -z $obs ]] && obs=200; id=$(like_to_id instance $id)
        if [[ -n $legacy ]]; then
                psql -U dp $import_db_name -c 'UPDATE instances SET instance=instance || JSONB('"'"'{"total_inserted_observations":'"${obs}}'"') WHERE instanceid = '"'$id'"
        else
                do_mongo_datasets "db.getCollection('$mongo_instances_c').update({id:'$id'}, {\$inc:{total_inserted_observations:$obs}})" | jq .
        fi
        inst $id
}
avro(){
    local inst_id=$1 job_id= file_url=; shift
    cd $ONS_DP_SRC_ROOT/dp-import || exit 2
    inst_id=$(like_to_id instance $inst_id); if [[ -z $inst_id ]]; then g_warn Cannot get instance id; return 1; fi
    job_id=$(inst_to_job $inst_id);  if [[ -z $job_id ]]; then g_warn Cannot get job_id from instance id $inst_id; return 1; fi
    file_url=$(job_to_file $job_id); if [[ -z $file_url ]]; then g_warn Cannot get file_url from job: $job_id instance: $inst_id; return 1; fi
    HUMAN_LOG=1 go run -race cmd/avro-kafka/main.go --kafka "$KAFKA_ADDR" --s3 "$file_url" --id "$inst_id" "$@"
}
dim_x(){ avro $1 --topic input-file-available; }
ins_ob(){ avro $1 --topic import-observations-inserted --inserts $2; }

list_db(){ node -e "$mongo_js;for(db in all){ console.log(db); }"; }
pg_all() {
    if   [[ -n $legacy ]]; then yorn --x psql -U dp $import_db_name -c 'SELECT * FROM jobs;'    -c 'SELECT * FROM instances;' -c 'SELECT * FROM dimensions;' -c 'SELECT COUNT(*) FROM dimensions;'
    else                        yorn --x psql -U dp $filter_db_name -c 'SELECT * FROM filters;' -c 'SELECT * FROM downloads;' -c 'SELECT * FROM dimensions;' -c 'SELECT COUNT(*) FROM dimensions;'
    fi
}
pg_wipe()   {
    if   [[ -n $legacy ]]; then yorn --x psql -U dp $import_db_name -f $import_api_dir/scripts/InitDatabase.sql
    else                        yorn --x psql -U dp $filter_db_name -f $filter_api_dir/scripts/InitDatabase.sql
    fi
}
# mongo_all_old (){ yorn --x do_mongo_r "db=db.getSiblingDB('$mongo_imports_db');p('coll $mongo_imports_c');r=db.getCollection('$mongo_imports_c').find();show_(r);db=db.getSiblingDB('$mongo_datasets_db');p('coll $mongo_instances_c');r=db.getCollection('$mongo_instances_c').find();show_(r)"; }
# mongo_all (){ yorn --xc do_mongo_all "if(d!='dimensions'){p(d);r=coll.find();show_(r)}"; yorn --xc do_mongo_all "if(d=='dimensions'){p(d);r=coll.find();show_(r)}"; }
mongo_all (){ local d=
    for d in $(list_db); do
        if [[ -n $1 ]]; then
            [[ " $@ " == *" $d "* ]] && do_mongo_all "if(d=='$d'){r=coll.find();show_(r)}"
        else
            yorn --comment "db $d" --xc do_mongo_all "if(d=='$d'){r=coll.find();show_(r)}"
        fi
    done
}
mongo_wipe(){ yorn --xc do_mongo_all "p(d);show_(coll.deleteMany({}));"; }
db     (){ mongo_all "$@";  pg_all;  neo_all; }
db_wipe(){ mongo_wipe; pg_wipe; neo_wipe; }
tester() { do_mongo_imports "db.getCollection('$mongo_imports_c').find({id:/^$1/}, {id:1})"; }
#tester() { like_to_id job 1d; }
health(){ for i in $IMPORT_API_ADDR $DATASET_API_ADDR $RECIPE_API_ADDR $FILTER_API_ADDR; do yorn --x api_raw_hdrs GET $i/healthcheck; done; }
ok(){ health "$@"; }

do=$1; shift; do=${do#--}
if   [[ :job:jobs: == *:$do:* ]]; then jobs_ "$@"
elif [[ $do == test           ]]; then
        eval $mongo_cmd $mongo_imports_db --eval \""db.getCollection('$mongo_imports_c').find()"\"
elif [[ :submitted:created: == *:$do:* ]]; then
        states jobs $do "$@"
        states instances $do "$@"
        states datasets $do "$@"
        states recipes $do "$@"
elif [[ :associated:published: == *:$do:* ]]; then
        states datasets $do "$@"
elif [[ :create:tester:inst:inst_to_job:inst_to_cols:inst_to_dims:inst_to_col1:inst_like:job_to_file: == *:$do:* \
            || :dset:assoc:filt:create_filt:create_filt_opt: == *:$do:* \
            || :tot_ob:db_ins_ob:ins_ob:submit:db_wipe:db:dim_x:neo:neo_count:neo_nodes:cypher_obs:mongo_all:mongo_wipe: == *:$do:* \
            || :add_file:health:ok:recipe: == *:$do:* ]]; then
        $do "$@"
elif [[ $do == help || $do == -h ]]; then
        usage
else
        usage >&2
        g_die 2 Do not recognise arg: $do
fi
res=$?; (( res > 0 )) && g_die $res "$do failed: $res"
